{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycaret.classification import *\n",
    "from pycaret.regression import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these file paths to match your files\n",
    "FEATURES_FILE1 = 'zeopp_final_results.csv'  # Your Zeo++ features\n",
    "FEATURES_FILE2 = '../kernelPCA_soap_mofs.csv'\n",
    "LABELS_FILE = '../id_labels.csv'           # Your labels file\n",
    "TARGET_COL = 'label'         # Your target variable\n",
    "\n",
    "select_f = [\"filename\"]\n",
    "# Load data\n",
    "print(\"Loading features...\")\n",
    "feat1 = pd.read_csv(FEATURES_FILE1)[select_f]\n",
    "feat1[\"filename\"] = feat1[\"filename\"].astype(str).str.strip()\n",
    "feat2 = pd.read_csv(FEATURES_FILE2)\n",
    "feat2[\"filename\"] = feat2[\"filename\"].astype(str).str.strip(\".cif\")\n",
    "features_df = pd.merge(feat1, feat2, on='filename', how='inner')\n",
    "\n",
    "# Clean whitespace from filename column\n",
    "features_df['filename'] = features_df['filename'].astype(str).str.strip()\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"Feature columns: {list(features_df.columns)}\")\n",
    "\n",
    "print(\"\\nLoading labels...\")\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "# Rename 'id' column to 'filename' to match features and clean whitespace\n",
    "labels_df = labels_df.rename(columns={'id': 'filename'})\n",
    "labels_df['filename'] = labels_df['filename'].astype(str).str.strip()\n",
    "print(f\"Labels shape: {labels_df.shape}\")\n",
    "print(f\"Label columns: {list(labels_df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of features:\")\n",
    "display(features_df.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of labels:\")\n",
    "display(labels_df.head())\n",
    "\n",
    "# Check if filenames match now\n",
    "print(\"\\nFilename matching check:\")\n",
    "print(\"Sample features filenames:\", features_df['filename'].head(3).tolist())\n",
    "print(\"Sample labels filenames:\", labels_df['filename'].head(3).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "print(\"Merging features and labels...\")\n",
    "merged_df = pd.merge(features_df, labels_df, on='filename', how='inner')\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "print(f\"Successfully merged {len(merged_df)} samples\")\n",
    "\n",
    "# Check for any missing merges\n",
    "feature_files = set(features_df['filename'])\n",
    "label_files = set(labels_df['filename'])\n",
    "print(f\"\\nFiles in features only: {len(feature_files - label_files)}\")\n",
    "print(f\"Files in labels only: {len(label_files - feature_files)}\")\n",
    "print(f\"Files in both: {len(feature_files.intersection(label_files))}\")\n",
    "\n",
    "display(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess features\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Get feature columns (exclude filename and target)\n",
    "feature_cols = [col for col in merged_df.columns if col not in ['filename', TARGET_COL]]\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "# Convert 'NA' strings to NaN and make numeric\n",
    "for col in feature_cols:\n",
    "    if merged_df[col].dtype == 'object':\n",
    "        merged_df[col] = merged_df[col].replace('NA', np.nan)\n",
    "        merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_counts = merged_df[feature_cols + [TARGET_COL]].isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])\n",
    "\n",
    "# Remove rows with missing target\n",
    "initial_rows = len(merged_df)\n",
    "merged_df = merged_df.dropna(subset=[TARGET_COL])\n",
    "final_rows = len(merged_df)\n",
    "print(f\"\\nRemoved {initial_rows - final_rows} rows with missing target values\")\n",
    "print(f\"Final dataset size: {final_rows} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset statistics:\")\n",
    "print(merged_df[feature_cols + [TARGET_COL]].describe())\n",
    "\n",
    "# Target variable analysis\n",
    "print(f\"\\nTarget variable: {TARGET_COL}\")\n",
    "if merged_df[TARGET_COL].dtype in ['int64', 'float64']:\n",
    "    print(\"Target statistics:\")\n",
    "    print(merged_df[TARGET_COL].describe())\n",
    "    \n",
    "    # Plot target distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    merged_df[TARGET_COL].hist(bins=30)\n",
    "    plt.title(f'{TARGET_COL} Distribution')\n",
    "    plt.xlabel(TARGET_COL)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(merged_df[TARGET_COL])\n",
    "    plt.title(f'{TARGET_COL} Boxplot')\n",
    "    plt.ylabel(TARGET_COL)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Target value counts:\")\n",
    "    print(merged_df[TARGET_COL].value_counts())\n",
    "    \n",
    "    # Plot target distribution\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    merged_df[TARGET_COL].value_counts().plot(kind='bar')\n",
    "    plt.title(f'{TARGET_COL} Distribution')\n",
    "    plt.xlabel(TARGET_COL)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset for machine learning\n",
    "ml_data = merged_df[[TARGET_COL] + feature_cols].copy()\n",
    "\n",
    "print(f\"ML dataset shape: {ml_data.shape}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Samples: {len(ml_data)}\")\n",
    "\n",
    "# Check final data quality\n",
    "print(\"\\nFinal data info:\")\n",
    "print(ml_data.info())\n",
    "\n",
    "# Determine task type\n",
    "target_unique = merged_df[TARGET_COL].nunique()\n",
    "if merged_df[TARGET_COL].dtype in ['int64', 'float64'] and target_unique > 10:\n",
    "    TASK_TYPE = 'regression'\n",
    "    print(f\"\\nDetected REGRESSION task (continuous target with {target_unique} unique values)\")\n",
    "else:\n",
    "    TASK_TYPE = 'classification'\n",
    "    print(f\"\\nDetected CLASSIFICATION task ({target_unique} classes)\")\n",
    "\n",
    "print(f\"Task type: {TASK_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if TASK_TYPE == 'regression'\n",
    "if TASK_TYPE == 'regression':\n",
    "    print(\"Setting up regression environment with normalization...\")\n",
    "    \n",
    "    reg = setup(\n",
    "            data=ml_data,\n",
    "            target=TARGET_COL,\n",
    "            train_size=0.8,\n",
    "            session_id=123,\n",
    "            # silent=True,\n",
    "            fold=5,\n",
    "            # Minimal preprocessing\n",
    "            normalize=True,          # Only keep normalization\n",
    "            transformation=False,    # Disable transformation\n",
    "            remove_outliers=False,   # Disable outlier removal\n",
    "            feature_selection=False, # Disable feature selection\n",
    "        )\n",
    "    \n",
    "    print(\"Regression setup complete with preprocessing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what preprocessing was applied\n",
    "print(\"=== PREPROCESSING SUMMARY ===\")\n",
    "print(\"\\nDataset shape after preprocessing:\")\n",
    "print(f\"Training features: {get_config('X_train').shape}\")\n",
    "print(f\"Test features: {get_config('X_test').shape}\")\n",
    "\n",
    "# Show what transformations were applied\n",
    "print(f\"\\nFeatures after preprocessing: {len(get_config('X_train').columns)}\")\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "\n",
    "if len(get_config('X_train').columns) != len(feature_cols):\n",
    "    print(f\"Feature selection removed: {len(feature_cols) - len(get_config('X_train').columns)} features\")\n",
    "\n",
    "# Show sample of processed data\n",
    "print(\"\\nSample of preprocessed training data:\")\n",
    "display(get_config('X_train').head())\n",
    "\n",
    "print(\"\\nPreprocessed data statistics:\")\n",
    "display(get_config('X_train').describe())\n",
    "\n",
    "# Check if normalization worked (should have mean~0, std~1)\n",
    "means = get_config('X_train').mean()\n",
    "stds = get_config('X_train').std()\n",
    "print(f\"\\nNormalization check:\")\n",
    "print(f\"Feature means range: {means.min():.3f} to {means.max():.3f} (should be near 0)\")\n",
    "print(f\"Feature stds range: {stds.min():.3f} to {stds.max():.3f} (should be near 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if TASK_TYPE == 'regression'\n",
    "if TASK_TYPE == 'regression':\n",
    "    print(\"Comparing regression models...\")\n",
    "    \n",
    "    # Compare multiple algorithms\n",
    "    best_models = compare_models(\n",
    "        include=['lr', 'rf', 'et', 'gbr', 'xgboost', 'dt'],\n",
    "        sort='R2',\n",
    "        n_select=5,  # Keep top 5 models\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"Model comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the best performing model\n",
    "print(\"Creating best model...\")\n",
    "best_model = create_model(best_models[0])\n",
    "\n",
    "# Tune hyperparameters\n",
    "print(\"\\nTuning hyperparameters...\")\n",
    "tuned_model = tune_model(best_model, optimize='R2' if TASK_TYPE == 'regression' else 'Accuracy')\n",
    "\n",
    "print(\"Model creation and tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned model\n",
    "print(\"Evaluating model performance...\")\n",
    "evaluate_model(tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"Analyzing feature importance...\")\n",
    "\n",
    "try:\n",
    "    # Try using PyCaret's built-in feature importance\n",
    "    if TASK_TYPE == 'classification':\n",
    "        plot_model(tuned_model, plot='feature', display_format='streamlit')\n",
    "    else:\n",
    "        plot_model(tuned_model, plot='feature', display_format='streamlit')\n",
    "except:\n",
    "    print(\"PyCaret feature importance not available for this model. Using alternative method...\")\n",
    "    \n",
    "    # Alternative: Extract feature importance from the model directly\n",
    "    try:\n",
    "        # Get the actual sklearn model\n",
    "        sklearn_model = tuned_model\n",
    "        \n",
    "        # Extract feature importance based on model type\n",
    "        if hasattr(sklearn_model, 'feature_importances_'):\n",
    "            # Tree-based models (RF, XGBoost, etc.)\n",
    "            importance_scores = sklearn_model.feature_importances_\n",
    "            importance_type = \"Tree-based Feature Importance\"\n",
    "        elif hasattr(sklearn_model, 'coef_'):\n",
    "            # Linear models\n",
    "            importance_scores = np.abs(sklearn_model.coef_).flatten()\n",
    "            importance_type = \"Coefficient Magnitude\"\n",
    "        else:\n",
    "            print(\"Feature importance not available for this model type\")\n",
    "            importance_scores = None\n",
    "        \n",
    "        if importance_scores is not None:\n",
    "            # Create feature importance DataFrame\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'Feature': feature_cols,\n",
    "                'Importance': importance_scores\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\n{importance_type}:\")\n",
    "            print(feature_importance_df.head(15))\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_features = feature_importance_df.head(15)\n",
    "            plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title(f'Top 15 Features - {importance_type}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Save feature importance\n",
    "            feature_importance_df.to_csv('feature_importance.csv', index=False)\n",
    "            print(f\"\\nFeature importance saved to: feature_importance.csv\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract feature importance: {e}\")\n",
    "        print(\"This might be a complex ensemble model where feature importance is not directly accessible\")\n",
    "\n",
    "# Alternative: Use permutation importance (works for any model)\n",
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    print(\"\\nCalculating permutation importance (this may take a moment)...\")\n",
    "    \n",
    "    # Get test data\n",
    "    X_test = get_config('X_test')\n",
    "    y_test = get_config('y_test')\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    perm_importance = permutation_importance(tuned_model, X_test, y_test, \n",
    "                                           n_repeats=5, random_state=123, n_jobs=-1)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance_Mean': perm_importance.importances_mean,\n",
    "        'Importance_Std': perm_importance.importances_std\n",
    "    }).sort_values('Importance_Mean', ascending=False)\n",
    "    \n",
    "    print(\"\\nPermutation Importance (Top 15):\")\n",
    "    print(perm_importance_df.head(15))\n",
    "    \n",
    "    # Plot permutation importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_perm_features = perm_importance_df.head(15)\n",
    "    plt.barh(range(len(top_perm_features)), top_perm_features['Importance_Mean'],\n",
    "             xerr=top_perm_features['Importance_Std'])\n",
    "    plt.yticks(range(len(top_perm_features)), top_perm_features['Feature'])\n",
    "    plt.xlabel('Permutation Importance')\n",
    "    plt.title('Top 15 Features - Permutation Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save permutation importance\n",
    "    perm_importance_df.to_csv('permutation_importance.csv', index=False)\n",
    "    print(f\"\\nPermutation importance saved to: permutation_importance.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate permutation importance: {e}\")\n",
    "\n",
    "print(\"\\nFeature importance analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on both training and test sets\n",
    "print(\"=== PREDICTIONS vs ACTUAL ===\")\n",
    "\n",
    "# Get test set predictions (this includes actual vs predicted)\n",
    "test_predictions = predict_model(tuned_model)\n",
    "print(\"Test set predictions:\")\n",
    "display(test_predictions.head(10))\n",
    "\n",
    "# Get training set predictions\n",
    "train_predictions = predict_model(tuned_model, data=get_config('X_train').assign(**{TARGET_COL: get_config('y_train')}))\n",
    "print(\"\\nTraining set predictions:\")\n",
    "display(train_predictions.head(10))\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "if TASK_TYPE == 'regression':\n",
    "    # For regression: scatter plots of actual vs predicted\n",
    "    \n",
    "    # Test set\n",
    "    axes[0,0].scatter(test_predictions[TARGET_COL], test_predictions['prediction_label'], alpha=0.6)\n",
    "    axes[0,0].plot([test_predictions[TARGET_COL].min(), test_predictions[TARGET_COL].max()], \n",
    "                   [test_predictions[TARGET_COL].min(), test_predictions[TARGET_COL].max()], 'r--', lw=2)\n",
    "    axes[0,0].set_xlabel('Actual')\n",
    "    axes[0,0].set_ylabel('Predicted')\n",
    "    axes[0,0].set_title('Test Set: Actual vs Predicted')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training set\n",
    "    axes[0,1].scatter(train_predictions[TARGET_COL], train_predictions['prediction_label'], alpha=0.6, color='orange')\n",
    "    axes[0,1].plot([train_predictions[TARGET_COL].min(), train_predictions[TARGET_COL].max()], \n",
    "                   [train_predictions[TARGET_COL].min(), train_predictions[TARGET_COL].max()], 'r--', lw=2)\n",
    "    axes[0,1].set_xlabel('Actual')\n",
    "    axes[0,1].set_ylabel('Predicted')\n",
    "    axes[0,1].set_title('Training Set: Actual vs Predicted')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals plots\n",
    "    test_residuals = test_predictions[TARGET_COL] - test_predictions['prediction_label']\n",
    "    train_residuals = train_predictions[TARGET_COL] - train_predictions['prediction_label']\n",
    "    \n",
    "    axes[1,0].scatter(test_predictions['prediction_label'], test_residuals, alpha=0.6)\n",
    "    axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1,0].set_xlabel('Predicted')\n",
    "    axes[1,0].set_ylabel('Residuals (Actual - Predicted)')\n",
    "    axes[1,0].set_title('Test Set: Residuals Plot')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1,1].scatter(train_predictions['prediction_label'], train_residuals, alpha=0.6, color='orange')\n",
    "    axes[1,1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1,1].set_xlabel('Predicted')\n",
    "    axes[1,1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "    axes[1,1].set_title('Training Set: Residuals Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Print metrics\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    print(f\"\\n=== REGRESSION METRICS ===\")\n",
    "    print(f\"Test Set:\")\n",
    "    print(f\"  MAE: {mean_absolute_error(test_predictions[TARGET_COL], test_predictions['prediction_label']):.4f}\")\n",
    "    print(f\"  RMSE: {np.sqrt(mean_squared_error(test_predictions[TARGET_COL], test_predictions['prediction_label'])):.4f}\")\n",
    "    print(f\"  R²: {r2_score(test_predictions[TARGET_COL], test_predictions['prediction_label']):.4f}\")\n",
    "    \n",
    "    print(f\"\\nTraining Set:\")\n",
    "    print(f\"  MAE: {mean_absolute_error(train_predictions[TARGET_COL], train_predictions['prediction_label']):.4f}\")\n",
    "    print(f\"  RMSE: {np.sqrt(mean_squared_error(train_predictions[TARGET_COL], train_predictions['prediction_label'])):.4f}\")\n",
    "    print(f\"  R²: {r2_score(train_predictions[TARGET_COL], train_predictions['prediction_label']):.4f}\")\n",
    "\n",
    "else:\n",
    "    # For classification: confusion matrices and distribution plots\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Test set confusion matrix\n",
    "    test_cm = confusion_matrix(test_predictions[TARGET_COL], test_predictions['prediction_label'])\n",
    "    sns.heatmap(test_cm, annot=True, fmt='d', ax=axes[0,0], cmap='Blues')\n",
    "    axes[0,0].set_title('Test Set: Confusion Matrix')\n",
    "    axes[0,0].set_xlabel('Predicted')\n",
    "    axes[0,0].set_ylabel('Actual')\n",
    "    \n",
    "    # Training set confusion matrix\n",
    "    train_cm = confusion_matrix(train_predictions[TARGET_COL], train_predictions['prediction_label'])\n",
    "    sns.heatmap(train_cm, annot=True, fmt='d', ax=axes[0,1], cmap='Oranges')\n",
    "    axes[0,1].set_title('Training Set: Confusion Matrix')\n",
    "    axes[0,1].set_xlabel('Predicted')\n",
    "    axes[0,1].set_ylabel('Actual')\n",
    "    \n",
    "    # Class distribution comparison\n",
    "    test_actual = test_predictions[TARGET_COL].value_counts().sort_index()\n",
    "    test_predicted = test_predictions['prediction_label'].value_counts().sort_index()\n",
    "    \n",
    "    x_pos = np.arange(len(test_actual))\n",
    "    width = 0.35\n",
    "    axes[1,0].bar(x_pos - width/2, test_actual.values, width, label='Actual', alpha=0.7)\n",
    "    axes[1,0].bar(x_pos + width/2, test_predicted.values, width, label='Predicted', alpha=0.7)\n",
    "    axes[1,0].set_xlabel('Classes')\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "    axes[1,0].set_title('Test Set: Class Distribution')\n",
    "    axes[1,0].set_xticks(x_pos)\n",
    "    axes[1,0].set_xticklabels(test_actual.index)\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Training set class distribution\n",
    "    train_actual = train_predictions[TARGET_COL].value_counts().sort_index()\n",
    "    train_predicted = train_predictions['prediction_label'].value_counts().sort_index()\n",
    "    \n",
    "    x_pos = np.arange(len(train_actual))\n",
    "    axes[1,1].bar(x_pos - width/2, train_actual.values, width, label='Actual', alpha=0.7)\n",
    "    axes[1,1].bar(x_pos + width/2, train_predicted.values, width, label='Predicted', alpha=0.7)\n",
    "    axes[1,1].set_xlabel('Classes')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    axes[1,1].set_title('Training Set: Class Distribution')\n",
    "    axes[1,1].set_xticks(x_pos)\n",
    "    axes[1,1].set_xticklabels(train_actual.index)\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    # Print classification reports\n",
    "    print(f\"\\n=== CLASSIFICATION METRICS ===\")\n",
    "    print(f\"Test Set Classification Report:\")\n",
    "    print(classification_report(test_predictions[TARGET_COL], test_predictions['prediction_label']))\n",
    "    \n",
    "    print(f\"\\nTraining Set Classification Report:\")\n",
    "    print(classification_report(train_predictions[TARGET_COL], train_predictions['prediction_label']))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions to CSV files\n",
    "test_predictions.to_csv('test_predictions.csv', index=False)\n",
    "train_predictions.to_csv('train_predictions.csv', index=False)\n",
    "print(f\"\\nPredictions saved:\")\n",
    "print(f\"- test_predictions.csv ({len(test_predictions)} samples)\")\n",
    "print(f\"- train_predictions.csv ({len(train_predictions)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyemma",
   "language": "python",
   "name": "pyemma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
